{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ca777c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['HADOOP_HOME'] = \"C:\\\\hadoop\"\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Pyspark 101 exercises\").getOrCreate()\n",
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e508e6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Name|Value|\n",
      "+-------+-----+\n",
      "|  Alice|    1|\n",
      "|    Bob|    2|\n",
      "|Charlie|    3|\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----+-----+\n",
      "|   Name|Value|index|\n",
      "+-------+-----+-----+\n",
      "|  Alice|    1|    0|\n",
      "|    Bob|    2|    1|\n",
      "|Charlie|    3|    2|\n",
      "+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "(\"Alice\", 1),\n",
    "(\"Bob\", 2),\n",
    "(\"Charlie\", 3),\n",
    "], [\"Name\", \"Value\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "\n",
    "df = df.withColumn(\"index\", row_number().over(w) - 1)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42457766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|Column1|Column2|\n",
      "+-------+-------+\n",
      "|      a|      1|\n",
      "|      b|      2|\n",
      "|      c|      3|\n",
      "|      d|      4|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list1 = [\"a\", \"b\", \"c\", \"d\"]\n",
    "list2 = [1, 2, 3, 4]\n",
    "rdd = spark.sparkContext.parallelize(list(zip(list1, list2)))\n",
    "df = rdd.toDF([\"Column1\", \"Column2\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbd0203a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "list_A = [1, 2, 3, 4, 5]\n",
    "list_B = [4, 5, 6, 7, 8]\n",
    "sc = spark.sparkContext\n",
    "\n",
    "rdd_A = sc.parallelize(list_A)\n",
    "rdd_B = sc.parallelize(list_B)\n",
    "\n",
    "result_rdd_A = rdd_A.subtract(rdd_B)\n",
    "result_rdd_B = rdd_B.subtract(rdd_A)\n",
    "\n",
    "result_rdd = result_rdd_A.union(result_rdd_B)\n",
    "\n",
    "result_list = result_rdd.collect()\n",
    "\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a8d6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|   A| 10|\n",
      "|   B| 20|\n",
      "|   C| 30|\n",
      "|   D| 40|\n",
      "|   E| 50|\n",
      "|   F| 15|\n",
      "|   G| 28|\n",
      "|   H| 54|\n",
      "|   I| 41|\n",
      "|   J| 86|\n",
      "+----+---+\n",
      "\n",
      "Min:  10.0\n",
      "25th percentile:  20.0\n",
      "Median:  30.0\n",
      "75th percentile:  50.0\n",
      "Max:  86.0\n"
     ]
    }
   ],
   "source": [
    "data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "df.show()\n",
    "quantiles = df.approxQuantile(\"Age\", [0.0, 0.25, 0.5, 0.75, 1.0], 0.01)\n",
    "\n",
    "print(\"Min: \", quantiles[0])\n",
    "print(\"25th percentile: \", quantiles[1])\n",
    "print(\"Median: \", quantiles[2])\n",
    "print(\"75th percentile: \", quantiles[3])\n",
    "print(\"Max: \", quantiles[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe958415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|name|      job|\n",
      "+----+---------+\n",
      "|John| Engineer|\n",
      "|John| Engineer|\n",
      "|Mary|Scientist|\n",
      "| Bob| Engineer|\n",
      "| Bob| Engineer|\n",
      "| Bob|Scientist|\n",
      "| Sam|   Doctor|\n",
      "+----+---------+\n",
      "\n",
      "+---------+-----+\n",
      "|      job|count|\n",
      "+---------+-----+\n",
      "| Engineer|    4|\n",
      "|Scientist|    2|\n",
      "|   Doctor|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='Mary', job='Scientist'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Scientist'),\n",
    "Row(name='Sam', job='Doctor'),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.show()\n",
    "df.groupBy(\"job\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ac79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|name|      job|\n",
      "+----+---------+\n",
      "|John| Engineer|\n",
      "|John| Engineer|\n",
      "|Mary|Scientist|\n",
      "| Bob| Engineer|\n",
      "| Bob| Engineer|\n",
      "| Bob|Scientist|\n",
      "| Sam|   Doctor|\n",
      "+----+---------+\n",
      "\n",
      "+----+---------+\n",
      "|name|      job|\n",
      "+----+---------+\n",
      "|John| Engineer|\n",
      "|John| Engineer|\n",
      "|Mary|Scientist|\n",
      "| Bob| Engineer|\n",
      "| Bob| Engineer|\n",
      "| Bob|Scientist|\n",
      "| Sam|    Other|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='Mary', job='Scientist'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Scientist'),\n",
    "Row(name='Sam', job='Doctor'),\n",
    "]\n",
    "\n",
    "# create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# show DataFrame\n",
    "df.show()\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "top_2_jobs = df.groupBy('job').count().orderBy('count', ascending=False).limit(2).select('job').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "df = df.withColumn('job', when(col('job').isin(top_2_jobs), col('job')).otherwise('Other'))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda8de16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+\n",
      "|Name|Value|  id|\n",
      "+----+-----+----+\n",
      "|   A|    1|NULL|\n",
      "|   B| NULL| 123|\n",
      "|   B|    3| 456|\n",
      "|   D| NULL|NULL|\n",
      "+----+-----+----+\n",
      "\n",
      "+----+-----+----+\n",
      "|Name|Value|  id|\n",
      "+----+-----+----+\n",
      "|   A|    1|NULL|\n",
      "|   B|    3| 456|\n",
      "+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df_2 = df.dropna(subset=['Value'])\n",
    "\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3130333e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   4|   5|   6|\n",
      "+----+----+----+\n",
      "\n",
      "+--------+--------+--------+\n",
      "|new_col1|new_col2|new_col3|\n",
      "+--------+--------+--------+\n",
      "|       1|       2|       3|\n",
      "|       4|       5|       6|\n",
      "+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# suppose you have the following DataFrame\n",
    "df = spark.createDataFrame([(1, 2, 3), (4, 5, 6)], [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "# old column names\n",
    "old_names = [\"col1\", \"col2\", \"col3\"]\n",
    "\n",
    "# new column names\n",
    "new_names = [\"new_col1\", \"new_col2\", \"new_col3\"]\n",
    "\n",
    "df.show()\n",
    "for old_name, new_name in zip(old_names, new_names):\n",
    "    df = df.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1978f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|             values|\n",
      "+-------------------+\n",
      "|  0.619189370225301|\n",
      "| 0.5096018842446481|\n",
      "| 0.8325259388871524|\n",
      "|0.26322809041172357|\n",
      "| 0.6702867696264135|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "+-------+-----+\n",
      "|buckets|count|\n",
      "+-------+-----+\n",
      "|    4.0|   10|\n",
      "|    3.0|   10|\n",
      "|    6.0|   10|\n",
      "|    9.0|   12|\n",
      "|    8.0|   10|\n",
      "|    2.0|   10|\n",
      "|    7.0|   10|\n",
      "|    5.0|   10|\n",
      "|    0.0|    8|\n",
      "|    1.0|   10|\n",
      "+-------+-----+\n",
      "\n",
      "+-------------------+-------+\n",
      "|             values|buckets|\n",
      "+-------------------+-------+\n",
      "|  0.619189370225301|    6.0|\n",
      "| 0.5096018842446481|    4.0|\n",
      "| 0.8325259388871524|    9.0|\n",
      "|0.26322809041172357|    3.0|\n",
      "| 0.6702867696264135|    6.0|\n",
      "+-------------------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "# Create a DataFrame with a single column \"values\" filled with random numbers\n",
    "num_items = 100\n",
    "df = spark.range(num_items).select(rand(seed=42).alias(\"values\"))\n",
    "\n",
    "df.show(5)\n",
    "num_buckets = 10\n",
    "quantiles = df.stat.approxQuantile(\"values\", [i/num_buckets for i in range(num_buckets+1)], 0.01)\n",
    "\n",
    "bucketizer = Bucketizer(splits=quantiles, inputCol=\"values\", outputCol=\"buckets\")\n",
    "\n",
    "df_buck = bucketizer.transform(df)\n",
    "\n",
    "df_buck.groupBy(\"buckets\").count().show()\n",
    "\n",
    "df_buck.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb86336e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|category1|category2|\n",
      "+---------+---------+\n",
      "|        A|        X|\n",
      "|        A|        Y|\n",
      "|        A|        X|\n",
      "|        B|        Y|\n",
      "|        B|        X|\n",
      "|        C|        X|\n",
      "|        C|        X|\n",
      "|        C|        Y|\n",
      "+---------+---------+\n",
      "\n",
      "+---------+-----+\n",
      "|category1|count|\n",
      "+---------+-----+\n",
      "|     NULL|    8|\n",
      "|        A|    3|\n",
      "|        B|    2|\n",
      "|        C|    3|\n",
      "+---------+-----+\n",
      "\n",
      "+-------------------+---+---+\n",
      "|category1_category2|  X|  Y|\n",
      "+-------------------+---+---+\n",
      "|                  B|  1|  1|\n",
      "|                  C|  2|  1|\n",
      "|                  A|  2|  1|\n",
      "+-------------------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"A\", \"X\"), (\"A\", \"Y\"), (\"A\", \"X\"), (\"B\", \"Y\"), (\"B\", \"X\"), (\"C\", \"X\"), (\"C\", \"X\"), (\"C\", \"Y\")]\n",
    "df = spark.createDataFrame(data, [\"category1\", \"category2\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.cube(\"category1\").count().show()\n",
    "df.crosstab('category1', 'category2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5b89c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|random|\n",
      "+---+------+\n",
      "|  0|     9|\n",
      "|  1|     8|\n",
      "|  2|     1|\n",
      "|  3|     4|\n",
      "|  4|     5|\n",
      "|  5|     1|\n",
      "|  6|    10|\n",
      "|  7|     1|\n",
      "|  8|    10|\n",
      "|  9|     6|\n",
      "+---+------+\n",
      "\n",
      "+---+------+----------------+\n",
      "| id|random|is_multiple_of_3|\n",
      "+---+------+----------------+\n",
      "|  0|     9|               1|\n",
      "|  1|     8|               0|\n",
      "|  2|     1|               0|\n",
      "|  3|     4|               0|\n",
      "|  4|     5|               0|\n",
      "|  5|     1|               0|\n",
      "|  6|    10|               0|\n",
      "|  7|     1|               0|\n",
      "|  8|    10|               0|\n",
      "|  9|     6|               1|\n",
      "+---+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand, col, when\n",
    "\n",
    "# Generate a DataFrame with a single column \"id\" with 10 rows\n",
    "df = spark.range(10)\n",
    "\n",
    "# Generate a random float between 0 and 1, scale and shift it to get a random integer between 1 and 10\n",
    "df = df.withColumn(\"random\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(\"is_multiple_of_3\", when(col(\"random\") % 3 == 0, 1).otherwise(0))\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be364102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|random|\n",
      "+---+------+\n",
      "|  0|     9|\n",
      "|  1|     8|\n",
      "|  2|     1|\n",
      "|  3|     4|\n",
      "|  4|     5|\n",
      "|  5|     1|\n",
      "|  6|    10|\n",
      "|  7|     1|\n",
      "|  8|    10|\n",
      "|  9|     6|\n",
      "+---+------+\n",
      "\n",
      "+---+------+-----+\n",
      "| id|random|index|\n",
      "+---+------+-----+\n",
      "|  0|     9|    0|\n",
      "|  1|     8|    1|\n",
      "|  2|     1|    2|\n",
      "|  3|     4|    3|\n",
      "|  4|     5|    4|\n",
      "|  5|     1|    5|\n",
      "|  6|    10|    6|\n",
      "|  7|     1|    7|\n",
      "|  8|    10|    8|\n",
      "|  9|     6|    9|\n",
      "+---+------+-----+\n",
      "\n",
      "+---+------+-----+\n",
      "| id|random|index|\n",
      "+---+------+-----+\n",
      "|  0|     9|    0|\n",
      "|  4|     5|    4|\n",
      "|  5|     1|    5|\n",
      "|  8|    10|    8|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand, row_number, monotonically_increasing_id\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Generate a DataFrame with a single column \"id\" with 10 rows\n",
    "df = spark.range(10)\n",
    "\n",
    "# Generate a random float between 0 and 1, scale and shift it to get a random integer between 1 and 10\n",
    "df = df.withColumn(\"random\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "pos = [0, 4, 8, 5]\n",
    "\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "df = df.withColumn(\"index\", row_number().over(w) - 1)\n",
    "\n",
    "df.show()\n",
    "df_filtered = df.filter(df.index.isin(pos))\n",
    "\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac99cb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|  Name|Col_1|Col_2|\n",
      "+------+-----+-----+\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   10|\n",
      "|orange|    2|    8|\n",
      "+------+-----+-----+\n",
      "\n",
      "+------+-----+-----+\n",
      "|  Name|Col_1|Col_3|\n",
      "+------+-----+-----+\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   15|\n",
      "| grape|    4|    6|\n",
      "+------+-----+-----+\n",
      "\n",
      "+------+-----+-----+\n",
      "|  Name|Col_1|Col_2|\n",
      "+------+-----+-----+\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   10|\n",
      "|orange|    2|    8|\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   15|\n",
      "| grape|    4|    6|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame for region A\n",
    "df_A = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 10), (\"orange\", 2, 8)], [\"Name\", \"Col_1\", \"Col_2\"])\n",
    "df_A.show()\n",
    "\n",
    "# Create DataFrame for region B\n",
    "df_B = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 15), (\"grape\", 4, 6)], [\"Name\", \"Col_1\", \"Col_3\"])\n",
    "df_B.show()\n",
    "df_A.union(df_B).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f6b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|actual|predicted|\n",
      "+------+---------+\n",
      "|     1|        1|\n",
      "|     2|        4|\n",
      "|     3|        9|\n",
      "|     4|       16|\n",
      "|     5|       25|\n",
      "+------+---------+\n",
      "\n",
      "Mean Squared Error (MSE) = 116.8\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]\n",
    "df = spark.createDataFrame(data, [\"actual\", \"predicted\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(\"squared_error\", pow((col(\"actual\") - col(\"predicted\")), 2))\n",
    "mse = df.agg({\"squared_error\": \"avg\"}).collect()[0][0]\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) = {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ee2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "| John|\n",
      "|Alice|\n",
      "|  Bob|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import initcap\n",
    "\n",
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "df = df.withColumn(\"name\", initcap(df[\"name\"]))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dcbba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|   name|age|salary|\n",
      "+-------+---+------+\n",
      "|  James| 34| 55000|\n",
      "|Michael| 30| 70000|\n",
      "| Robert| 37| 60000|\n",
      "|  Maria| 29| 80000|\n",
      "|    Jen| 32| 65000|\n",
      "+-------+---+------+\n",
      "\n",
      "+-------+------+-----------------+-----------------+\n",
      "|summary|  name|              age|           salary|\n",
      "+-------+------+-----------------+-----------------+\n",
      "|  count|     5|                5|                5|\n",
      "|   mean|  NULL|             32.4|          66000.0|\n",
      "| stddev|  NULL|3.209361307176242|9617.692030835675|\n",
      "|    min| James|               29|            55000|\n",
      "|    25%|  NULL|               30|            60000|\n",
      "|    50%|  NULL|               32|            65000|\n",
      "|    75%|  NULL|               34|            70000|\n",
      "|    max|Robert|               37|            80000|\n",
      "+-------+------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James', 34, 55000),\n",
    "('Michael', 30, 70000),\n",
    "('Robert', 37, 60000),\n",
    "('Maria', 29, 80000),\n",
    "('Jen', 32, 65000)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "summary = df.summary()\n",
    "summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a90797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "| john|\n",
      "|alice|\n",
      "|  bob|\n",
      "+-----+\n",
      "\n",
      "+-----+-----------+\n",
      "| name|word_length|\n",
      "+-----+-----------+\n",
      "| john|          4|\n",
      "|alice|          5|\n",
      "|  bob|          3|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn('word_length', F.length(df.name))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073884bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|   name|age|salary|\n",
      "+-------+---+------+\n",
      "|  James| 34| 55000|\n",
      "|Michael| 30| 70000|\n",
      "| Robert| 37| 60000|\n",
      "|  Maria| 29| 80000|\n",
      "|    Jen| 32| 65000|\n",
      "+-------+---+------+\n",
      "\n",
      "+-------+---+------+----------+------+\n",
      "|   name|age|salary|prev_value|  diff|\n",
      "+-------+---+------+----------+------+\n",
      "|  James| 34| 55000|      NULL|     0|\n",
      "|Michael| 30| 70000|     55000| 15000|\n",
      "| Robert| 37| 60000|     70000|-10000|\n",
      "|  Maria| 29| 80000|     60000| 20000|\n",
      "|    Jen| 32| 65000|     80000|-15000|\n",
      "+-------+---+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data = [('James', 34, 55000),\n",
    "('Michael', 30, 70000),\n",
    "('Robert', 37, 60000),\n",
    "('Maria', 29, 80000),\n",
    "('Jen', 32, 65000)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
    "\n",
    "df.show()\n",
    "df = df.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "window = Window.orderBy(\"id\")\n",
    "\n",
    "df = df.withColumn(\"prev_value\", F.lag(df.salary).over(window))\n",
    "\n",
    "df = df.withColumn(\"diff\", F.when(F.isnull(df.salary - df.prev_value), 0)\n",
    ".otherwise(df.salary - df.prev_value)).drop(\"id\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f669f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|date_str_1| date_str_2|\n",
      "+----------+-----------+\n",
      "|2023-05-18|01 Jan 2010|\n",
      "|2023-12-31|01 Jan 2010|\n",
      "+----------+-----------+\n",
      "\n",
      "+----------+-----------+----------+----------+------------+-----------+-----------+-----------+\n",
      "|date_str_1| date_str_2|    date_1|    date_2|day_of_month|week_number|day_of_year|day_of_week|\n",
      "+----------+-----------+----------+----------+------------+-----------+-----------+-----------+\n",
      "|2023-05-18|01 Jan 2010|2023-05-18|2010-01-01|          18|         20|        138|          5|\n",
      "|2023-12-31|01 Jan 2010|2023-12-31|2010-01-01|          31|         52|        365|          1|\n",
      "+----------+-----------+----------+----------+------------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, dayofmonth, weekofyear, dayofyear, dayofweek\n",
    "\n",
    "data = [(\"2023-05-18\",\"01 Jan 2010\",), (\"2023-12-31\", \"01 Jan 2010\",)]\n",
    "df = spark.createDataFrame(data, [\"date_str_1\", \"date_str_2\"])\n",
    "\n",
    "df.show()\n",
    "df = df.withColumn(\"date_1\", to_date(df.date_str_1, 'yyyy-MM-dd'))\n",
    "df = df.withColumn(\"date_2\", to_date(df.date_str_2, 'dd MMM yyyy'))\n",
    "\n",
    "df = df.withColumn(\"day_of_month\", dayofmonth(df.date_1))\\\n",
    ".withColumn(\"week_number\", weekofyear(df.date_1))\\\n",
    ".withColumn(\"day_of_year\", dayofyear(df.date_1))\\\n",
    ".withColumn(\"day_of_week\", dayofweek(df.date_1))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbebbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|MonthYear|\n",
      "+---------+\n",
      "| Jan 2010|\n",
      "| Feb 2011|\n",
      "| Mar 2012|\n",
      "+---------+\n",
      "\n",
      "+---------+----------+\n",
      "|MonthYear|      Date|\n",
      "+---------+----------+\n",
      "| Jan 2010|2010-01-01|\n",
      "| Feb 2011|2011-02-01|\n",
      "| Mar 2012|2012-03-01|\n",
      "+---------+----------+\n",
      "\n",
      "+---------+----------+\n",
      "|MonthYear|      Date|\n",
      "+---------+----------+\n",
      "| Jan 2010|2010-01-04|\n",
      "| Feb 2011|2011-02-04|\n",
      "| Mar 2012|2012-03-04|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "df = spark.createDataFrame([('Jan 2010',), ('Feb 2011',), ('Mar 2012',)], ['MonthYear'])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn('Date', expr(\"to_date(MonthYear, 'MMM yyyy')\"))\n",
    "\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn('Date', expr(\"date_add(date_sub(Date, day(Date) - 1), 3)\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810888a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  Word|\n",
      "+------+\n",
      "| Apple|\n",
      "|Orange|\n",
      "|  Plan|\n",
      "|Python|\n",
      "| Money|\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "|  Word|\n",
      "+------+\n",
      "| Apple|\n",
      "|Orange|\n",
      "| Money|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, length, translate\n",
    "df = spark.createDataFrame([('Apple',), ('Orange',), ('Plan',) , ('Python',) , ('Money',)], ['Word'])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df_filtered = df.where((length(col('Word')) - length(translate(col('Word'), 'AEIOUaeiou', ''))) >= 2)\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4354930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|value                     |\n",
      "+--------------------------+\n",
      "|buying books at amazom.com|\n",
      "|rameses@egypt.com         |\n",
      "|matt@t.co                 |\n",
      "|narendra@modi.com         |\n",
      "+--------------------------+\n",
      "\n",
      "+-----------------+\n",
      "|            value|\n",
      "+-----------------+\n",
      "|rameses@egypt.com|\n",
      "|        matt@t.co|\n",
      "|narendra@modi.com|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = ['buying books at amazom.com', 'rameses@egypt.com', 'matt@t.co', 'narendra@modi.com']\n",
    "\n",
    "df = spark.createDataFrame(data, \"string\")\n",
    "df.show(truncate =False)\n",
    "\n",
    "pattern = \"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n",
    "\n",
    "df_filtered = df.filter(F.col(\"value\").rlike(pattern))\n",
    "\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff21cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+-------+\n",
      "|year|quarter|region|revenue|\n",
      "+----+-------+------+-------+\n",
      "|2021|      1|    US|   5000|\n",
      "|2021|      1|    EU|   4000|\n",
      "|2021|      2|    US|   5500|\n",
      "|2021|      2|    EU|   4500|\n",
      "|2021|      3|    US|   6000|\n",
      "|2021|      3|    EU|   5000|\n",
      "|2021|      4|    US|   7000|\n",
      "|2021|      4|    EU|   6000|\n",
      "+----+-------+------+-------+\n",
      "\n",
      "+----+-------+----+----+\n",
      "|year|quarter|  EU|  US|\n",
      "+----+-------+----+----+\n",
      "|2021|      2|4500|5500|\n",
      "|2021|      1|4000|5000|\n",
      "|2021|      3|5000|6000|\n",
      "|2021|      4|6000|7000|\n",
      "+----+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "(2021, 1, \"US\", 5000),\n",
    "(2021, 1, \"EU\", 4000),\n",
    "(2021, 2, \"US\", 5500),\n",
    "(2021, 2, \"EU\", 4500),\n",
    "(2021, 3, \"US\", 6000),\n",
    "(2021, 3, \"EU\", 5000),\n",
    "(2021, 4, \"US\", 7000),\n",
    "(2021, 4, \"EU\", 6000),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"year\", \"quarter\", \"region\", \"revenue\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "pivot_df = df.groupBy(\"year\", \"quarter\").pivot(\"region\").sum(\"revenue\")\n",
    "\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04860d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+\n",
      "|OrderID|   Product|Price|\n",
      "+-------+----------+-----+\n",
      "|   1001|    Laptop| 1000|\n",
      "|   1002|     Mouse|   50|\n",
      "|   1003|    Laptop| 1200|\n",
      "|   1004|     Mouse|   30|\n",
      "|   1005|Smartphone|  700|\n",
      "+-------+----------+-----+\n",
      "\n",
      "+----------+-----------+\n",
      "|   Product|Total_Sales|\n",
      "+----------+-----------+\n",
      "|    Laptop|     1100.0|\n",
      "|     Mouse|       40.0|\n",
      "|Smartphone|      700.0|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "# Sample data\n",
    "data = [(\"1001\", \"Laptop\", 1000),\n",
    "(\"1002\", \"Mouse\", 50),\n",
    "(\"1003\", \"Laptop\", 1200),\n",
    "(\"1004\", \"Mouse\", 30),\n",
    "(\"1005\", \"Smartphone\", 700)]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"OrderID\", \"Product\", \"Price\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()\n",
    "\n",
    "result = df.groupBy(\"Product\").agg(mean(\"Price\").alias(\"Total_Sales\"))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec6f68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|series1|series2|\n",
      "+-------+-------+\n",
      "|      1|     10|\n",
      "|      2|      9|\n",
      "|      3|      8|\n",
      "|      4|      7|\n",
      "|      5|      6|\n",
      "|      6|      5|\n",
      "|      7|      4|\n",
      "|      8|      3|\n",
      "|      9|      2|\n",
      "|     10|      1|\n",
      "+-------+-------+\n",
      "\n",
      "+------------------+\n",
      "|euclidean_distance|\n",
      "+------------------+\n",
      "| 18.16590212458495|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "data = [(1, 10), (2, 9), (3, 8), (4, 7), (5, 6), (6, 5), (7, 4), (8, 3), (9, 2), (10, 1)]\n",
    "\n",
    "# Convert list to DataFrame\n",
    "df = spark.createDataFrame(data, [\"series1\", \"series2\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=[\"series1\", \"series2\"], outputCol=\"vectors\")\n",
    "df = vecAssembler.transform(df)\n",
    "\n",
    "df = df.withColumn(\"squared_diff\", expr(\"POW(series1 - series2, 2)\"))\n",
    "\n",
    "df.agg(expr(\"SQRT(SUM(squared_diff))\").alias(\"euclidean_distance\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6def984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|           string|\n",
      "+-----------------+\n",
      "|dbc deb abed gade|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+-----------------+\n",
      "|           string|  modified_string|\n",
      "+-----------------+-----------------+\n",
      "|dbc deb abed gade|dbccdebcabedcgade|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from collections import Counter\n",
    "#Sample DataFrame\n",
    "df = spark.createDataFrame([('dbc deb abed gade',),], [\"string\"])\n",
    "df.show()\n",
    "\n",
    "def least_freq_char_replace_spaces(s):\n",
    "    counter = Counter(s.replace(\" \", \"\"))\n",
    "    least_freq_char = min(counter, key = counter.get)\n",
    "    return s.replace(' ', least_freq_char)\n",
    "\n",
    "udf_least_freq_char_replace_spaces = udf(least_freq_char_replace_spaces, StringType())\n",
    "\n",
    "df = spark.createDataFrame([('dbc deb abed gade',)], [\"string\"])\n",
    "df.withColumn('modified_string', udf_least_freq_char_replace_spaces(df['string'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4779d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|      date|random_numbers|\n",
      "+----------+--------------+\n",
      "|2000-01-01|             6|\n",
      "|2000-01-08|             1|\n",
      "|2000-01-15|             7|\n",
      "|2000-01-22|             7|\n",
      "|2000-01-29|             2|\n",
      "|2000-02-05|             4|\n",
      "|2000-02-12|             3|\n",
      "|2000-02-19|             8|\n",
      "|2000-02-26|             1|\n",
      "|2000-03-04|             5|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, explode, sequence, rand\n",
    "\n",
    "start_date = '2000-01-01'\n",
    "end_date = '2000-03-04' \n",
    "\n",
    "df = spark.range(1).select(\n",
    "explode(\n",
    "sequence(\n",
    "expr(f\"date '{start_date}'\"),\n",
    "expr(f\"date '{end_date}'\"),\n",
    "expr(\"interval 1 day\")\n",
    ")\n",
    ").alias(\"date\")\n",
    ")\n",
    "\n",
    "df = df.filter(expr(\"dayofweek(date) = 7\"))\n",
    "df = df.withColumn(\"random_numbers\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4c61c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
      "|RowNumber|CustomerId|Surname |CreditScore|Geography|Gender|Age|Tenure|Balance  |NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|Exited|\n",
      "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
      "|1        |15634602  |Hargrave|619        |France   |Female|42 |2     |0.0      |1            |1        |1             |101348.88      |1     |\n",
      "|2        |15647311  |Hill    |608        |Spain    |Female|41 |1     |83807.86 |1            |0        |1             |112542.58      |0     |\n",
      "|3        |15619304  |Onio    |502        |France   |Female|42 |8     |159660.8 |3            |1        |0             |113931.57      |1     |\n",
      "|4        |15701354  |Boni    |699        |France   |Female|39 |1     |0.0      |2            |0        |0             |93826.63       |0     |\n",
      "|5        |15737888  |Mitchell|850        |Spain    |Female|43 |2     |125510.82|1            |1        |1             |79084.1        |0     |\n",
      "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Number of Rows:  10000\n",
      "Number of Columns:  14\n",
      "Data types:  [('RowNumber', 'int'), ('CustomerId', 'int'), ('Surname', 'string'), ('CreditScore', 'int'), ('Geography', 'string'), ('Gender', 'string'), ('Age', 'int'), ('Tenure', 'int'), ('Balance', 'double'), ('NumOfProducts', 'int'), ('HasCrCard', 'int'), ('IsActiveMember', 'int'), ('EstimatedSalary', 'double'), ('Exited', 'int')]\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/selva86/datasets/master/Churn_Modelling.csv\"\n",
    "\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "df = spark.read.csv(SparkFiles.get(\"Churn_Modelling.csv\"), header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "nrows = df.count()\n",
    "print(\"Number of Rows: \", nrows)\n",
    "\n",
    "ncols = len(df.columns)\n",
    "print(\"Number of Columns: \", ncols)\n",
    "\n",
    "datatypes = df.dtypes\n",
    "print(\"Data types: \", datatypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d665125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+\n",
      "| name|age|qty|\n",
      "+-----+---+---+\n",
      "|Alice|  1| 30|\n",
      "|  Bob|  2| 35|\n",
      "+-----+---+---+\n",
      "\n",
      "+-----+--------+--------+\n",
      "| name|user_age|user_qty|\n",
      "+-----+--------+--------+\n",
      "|Alice|       1|      30|\n",
      "|  Bob|       2|      35|\n",
      "+-----+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('Alice', 1, 30),('Bob', 2, 35)], [\"name\", \"age\", \"qty\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "old_names = [\"qty\", \"age\"]\n",
    "new_names = [\"user_qty\", \"user_age\"]\n",
    "\n",
    "for old_name, new_name in zip(old_names, new_names):\n",
    "    df = df.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd53ca79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+\n",
      "|Name|Value|  id|\n",
      "+----+-----+----+\n",
      "|   A|    1|NULL|\n",
      "|   B| NULL| 123|\n",
      "|   B|    3| 456|\n",
      "|   D| NULL|NULL|\n",
      "+----+-----+----+\n",
      "\n",
      "True\n",
      "{'Name': 0, 'Value': 2, 'id': 2}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "missing = df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns))\n",
    "has_missing = any(row.asDict().values() for row in missing.collect())\n",
    "print(has_missing)\n",
    "\n",
    "missing_count = missing.collect()[0].asDict()\n",
    "print(missing_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a47ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|Name|var1|var2|\n",
      "+----+----+----+\n",
      "|   A|   1|NULL|\n",
      "|   B|NULL| 123|\n",
      "|   B|   3| 456|\n",
      "|   D|   6|NULL|\n",
      "+----+----+----+\n",
      "\n",
      "+----+----+----+\n",
      "|Name|var1|var2|\n",
      "+----+----+----+\n",
      "|   A|   1| 289|\n",
      "|   B|   3| 123|\n",
      "|   B|   3| 456|\n",
      "|   D|   6| 289|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, 123 ),\n",
    "(\"B\", 3, 456),\n",
    "(\"D\", 6, None),\n",
    "], [\"Name\", \"var1\", \"var2\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "column_names = [\"var1\", \"var2\"]\n",
    "imputer = Imputer(inputCols= column_names, outputCols= column_names, strategy=\"mean\")\n",
    "model = imputer.fit(df)\n",
    "imputed_df = model.transform(df)\n",
    "\n",
    "imputed_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803dc3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+\n",
      "|First_Name|Last_Name|Age|\n",
      "+----------+---------+---+\n",
      "|      John|      Doe| 30|\n",
      "|      Jane|      Doe| 25|\n",
      "|     Alice|    Smith| 22|\n",
      "+----------+---------+---+\n",
      "\n",
      "+---+----------+---------+\n",
      "|Age|First_Name|Last_Name|\n",
      "+---+----------+---------+\n",
      "| 30|      John|      Doe|\n",
      "| 25|      Jane|      Doe|\n",
      "| 22|     Alice|    Smith|\n",
      "+---+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"John\", \"Doe\", 30), (\"Jane\", \"Doe\", 25), (\"Alice\", \"Smith\", 22)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"First_Name\", \"Last_Name\", \"Age\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "new_order = [\"Age\", \"First_Name\", \"Last_Name\"]\n",
    "\n",
    "df = df.select(*new_order)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c10b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|your_column|\n",
      "+---+-----------+\n",
      "|  1|    1.23E-7|\n",
      "|  2|  2.3456E-5|\n",
      "|  3| 3.45678E-4|\n",
      "+---+-----------+\n",
      "\n",
      "+---+------------+\n",
      "| id| your_column|\n",
      "+---+------------+\n",
      "|  1|0.0000001230|\n",
      "|  2|0.0000234560|\n",
      "|  3|0.0003456780|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "df = spark.createDataFrame([(1, 0.000000123), (2, 0.000023456), (3, 0.000345678)], [\"id\", \"your_column\"])\n",
    "df.show()\n",
    "\n",
    "decimal_places = 10\n",
    "\n",
    "df = df.withColumn(\"your_column\", format_number(\"your_column\", decimal_places))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c2f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|numbers_1|numbers_2|\n",
      "+---------+---------+\n",
      "|      0.1|     0.08|\n",
      "|      0.2|     0.06|\n",
      "|     0.33|     0.02|\n",
      "+---------+---------+\n",
      "\n",
      "+---------+---------+\n",
      "|numbers_1|numbers_2|\n",
      "+---------+---------+\n",
      "|   10.00%|    8.00%|\n",
      "|   20.00%|    6.00%|\n",
      "|   33.00%|    2.00%|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "data = [(0.1, .08), (0.2, .06), (0.33, .02)]\n",
    "df = spark.createDataFrame(data, [\"numbers_1\", \"numbers_2\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "columns = [\"numbers_1\", \"numbers_2\"]\n",
    "\n",
    "for col_name in columns:\n",
    "    df = df.withColumn(col_name, concat((col(col_name) * 100).cast('decimal(10, 2)'), lit(\"%\")))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bd7a08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   Name|Number|\n",
      "+-------+------+\n",
      "|  Alice|     1|\n",
      "|    Bob|     2|\n",
      "|Charlie|     3|\n",
      "|   Dave|     4|\n",
      "|    Eve|     5|\n",
      "|  Frank|     6|\n",
      "|  Grace|     7|\n",
      "| Hannah|     8|\n",
      "|   Igor|     9|\n",
      "|   Jack|    10|\n",
      "+-------+------+\n",
      "\n",
      "+----+------+---+\n",
      "|Name|Number| rn|\n",
      "+----+------+---+\n",
      "| Eve|     5|  5|\n",
      "|Jack|    10| 10|\n",
      "+----+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3), (\"Dave\", 4), (\"Eve\", 5),\n",
    "(\"Frank\", 6), (\"Grace\", 7), (\"Hannah\", 8), (\"Igor\", 9), (\"Jack\", 10)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Number\"])\n",
    "\n",
    "df.show()\n",
    "window = Window.orderBy(monotonically_increasing_id())\n",
    "df = df.withColumn(\"rn\", row_number().over(window))\n",
    "n = 5 \n",
    "df = df.filter((df.rn % n) == 0)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdc33236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|column1|\n",
      "+---+-------+\n",
      "|  1|      5|\n",
      "|  2|      8|\n",
      "|  3|     12|\n",
      "|  4|      1|\n",
      "|  5|     15|\n",
      "|  6|      7|\n",
      "+---+-------+\n",
      "\n",
      "Row number: 5\n",
      "Column value: 5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc, row_number\n",
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "Row(id=1, column1=5),\n",
    "Row(id=2, column1=8),\n",
    "Row(id=3, column1=12),\n",
    "Row(id=4, column1=1),\n",
    "Row(id=5, column1=15),\n",
    "Row(id=6, column1=7),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n",
    "\n",
    "window = Window.orderBy(desc(\"column1\"))\n",
    "df = df.withColumn(\"row_number\", row_number().over(window))\n",
    "\n",
    "row = df.filter(df.row_number == n).first()\n",
    "\n",
    "if row:\n",
    "    print(\"Row number:\", row.row_number)\n",
    "    print(\"Column value:\", row.column1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20acc9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|  10|  25|  70|\n",
      "|  40|   5|  20|\n",
      "|  70|  80| 100|\n",
      "|  10|   2|  60|\n",
      "|  40|  50|  20|\n",
      "+----+----+----+\n",
      "\n",
      "+----+----+----+-------+\n",
      "|col1|col2|col3|row_sum|\n",
      "+----+----+----+-------+\n",
      "|  10|  25|  70|    105|\n",
      "|  40|   5|  20|     65|\n",
      "|  70|  80| 100|    250|\n",
      "|  10|   2|  60|     72|\n",
      "|  40|  50|  20|    110|\n",
      "+----+----+----+-------+\n",
      "\n",
      "+----+----+----+-------+\n",
      "|col1|col2|col3|row_sum|\n",
      "+----+----+----+-------+\n",
      "|  10|  25|  70|    105|\n",
      "|  70|  80| 100|    250|\n",
      "|  40|  50|  20|    110|\n",
      "+----+----+----+-------+\n",
      "\n",
      "+----+----+----+-------+------------+\n",
      "|col1|col2|col3|row_sum|          id|\n",
      "+----+----+----+-------+------------+\n",
      "|  40|  50|  20|    110|128849018880|\n",
      "|  70|  80| 100|    250| 77309411328|\n",
      "+----+----+----+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "data = [(10, 25, 70),\n",
    "(40, 5, 20),\n",
    "(70, 80, 100),\n",
    "(10, 2, 60),\n",
    "(40, 50, 20)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn('row_sum', reduce(lambda a, b: a+b, [F.col(x) for x in df.columns]))\n",
    "\n",
    "df.show()\n",
    "\n",
    "df = df.filter(F.col('row_sum') > 100)\n",
    "\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn('id', F.monotonically_increasing_id())\n",
    "\n",
    "df_last_2 = df.sort(F.desc('id')).limit(2)\n",
    "\n",
    "df_last_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a7951b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   4|   5|   6|\n",
      "|   7|   8|   9|\n",
      "|  10|  11|  12|\n",
      "+----+----+----+\n",
      "\n",
      "+----+----+----+----------+\n",
      "|col1|col2|col3|min_by_max|\n",
      "+----+----+----+----------+\n",
      "|   1|   2|   3|0.33333334|\n",
      "|   4|   5|   6| 0.6666667|\n",
      "|   7|   8|   9| 0.7777778|\n",
      "|  10|  11|  12| 0.8333333|\n",
      "+----+----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, array\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "data = [(1, 2, 3), (4, 5, 6), (7, 8, 9), (10, 11, 12)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "def min_max_ratio(row):\n",
    "    return float(min(row)) / max(row)\n",
    "\n",
    "min_max_ratio_udf = udf(min_max_ratio, FloatType())\n",
    "\n",
    "df = df.withColumn('min_by_max', min_max_ratio_udf(array(df.columns)))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a11010e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|Column1|Column2|Column3|\n",
      "+-------+-------+-------+\n",
      "|     10|     20|     30|\n",
      "|     40|     60|     50|\n",
      "|     80|     70|     90|\n",
      "+-------+-------+-------+\n",
      "\n",
      "+-------+-------+-------+-----------+\n",
      "|Column1|Column2|Column3|Penultimate|\n",
      "+-------+-------+-------+-----------+\n",
      "|     10|     20|     30|         20|\n",
      "|     40|     60|     50|         50|\n",
      "|     80|     70|     90|         80|\n",
      "+-------+-------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "\n",
    "data = [(10, 20, 30),\n",
    "(40, 60, 50),\n",
    "(80, 70, 90)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Column1\", \"Column2\", \"Column3\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "sort_array_desc = F.udf(lambda arr: sorted(arr), ArrayType(IntegerType()))\n",
    "\n",
    "df = df.withColumn(\"row_as_array\", sort_array_desc(F.array(df.columns)))\n",
    "df = df.withColumn(\"Penultimate\", df['row_as_array'].getItem(1))\n",
    "df = df.drop('row_as_array')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d2be325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|Col1|Col2|Col3|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   2|   3|   4|\n",
      "|   3|   4|   5|\n",
      "|   4|   5|   6|\n",
      "+----+----+----+\n",
      "\n",
      "+----+----+----+-------------------------------------------------------------+\n",
      "|Col1|Col2|Col3|scaled_features                                              |\n",
      "+----+----+----+-------------------------------------------------------------+\n",
      "|1   |2   |3   |[-1.161895003862225,-1.161895003862225,-1.161895003862225]   |\n",
      "|2   |3   |4   |[-0.3872983346207417,-0.3872983346207417,-0.3872983346207417]|\n",
      "|3   |4   |5   |[0.3872983346207417,0.3872983346207417,0.3872983346207417]   |\n",
      "|4   |5   |6   |[1.161895003862225,1.161895003862225,1.161895003862225]      |\n",
      "+----+----+----+-------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "data = [(1, 2, 3),\n",
    "(2, 3, 4),\n",
    "(3, 4, 5),\n",
    "(4, 5, 6)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Col1\", \"Col2\", \"Col3\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "input_cols = [\"Col1\", \"Col2\", \"Col3\"]\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "df_assembled = assembler.transform(df)\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "scalerModel = scaler.fit(df_assembled)\n",
    "df_normalized = scalerModel.transform(df_assembled)\n",
    "df_normalized = df_normalized.drop('features')\n",
    "\n",
    "df_normalized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5830aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Name1|Name2|\n",
      "+-----+-----+\n",
      "| John| John|\n",
      "| Lily| Lucy|\n",
      "|  Sam|  Sam|\n",
      "| Lucy| Lily|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+-----+\n",
      "|Name1|Name2|Match|\n",
      "+-----+-----+-----+\n",
      "| John| John| true|\n",
      "| Lily| Lucy|false|\n",
      "|  Sam|  Sam| true|\n",
      "| Lucy| Lily|false|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "data = [(\"John\", \"John\"), (\"Lily\", \"Lucy\"), (\"Sam\", \"Sam\"), (\"Lucy\", \"Lily\")]\n",
    "df = spark.createDataFrame(data, [\"Name1\", \"Name2\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(\"Match\", when(col(\"Name1\") == col(\"Name2\"), True).otherwise(False))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7fe2aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n",
      "|      Date| Store|Sales|\n",
      "+----------+------+-----+\n",
      "|2023-01-01|Store1|  100|\n",
      "|2023-01-02|Store1|  150|\n",
      "|2023-01-03|Store1|  200|\n",
      "|2023-01-04|Store1|  250|\n",
      "|2023-01-05|Store1|  300|\n",
      "|2023-01-01|Store2|   50|\n",
      "|2023-01-02|Store2|   60|\n",
      "|2023-01-03|Store2|   80|\n",
      "|2023-01-04|Store2|   90|\n",
      "|2023-01-05|Store2|  120|\n",
      "+----------+------+-----+\n",
      "\n",
      "+----------+------+-----+---------+----------+\n",
      "|      Date| Store|Sales|Lag_Sales|Lead_Sales|\n",
      "+----------+------+-----+---------+----------+\n",
      "|2023-01-01|Store1|  100|     NULL|       150|\n",
      "|2023-01-02|Store1|  150|      100|       200|\n",
      "|2023-01-03|Store1|  200|      150|       250|\n",
      "|2023-01-04|Store1|  250|      200|       300|\n",
      "|2023-01-05|Store1|  300|      250|      NULL|\n",
      "|2023-01-01|Store2|   50|     NULL|        60|\n",
      "|2023-01-02|Store2|   60|       50|        80|\n",
      "|2023-01-03|Store2|   80|       60|        90|\n",
      "|2023-01-04|Store2|   90|       80|       120|\n",
      "|2023-01-05|Store2|  120|       90|      NULL|\n",
      "+----------+------+-----+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag, lead, to_date\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [(\"2023-01-01\", \"Store1\", 100),\n",
    "(\"2023-01-02\", \"Store1\", 150),\n",
    "(\"2023-01-03\", \"Store1\", 200),\n",
    "(\"2023-01-04\", \"Store1\", 250),\n",
    "(\"2023-01-05\", \"Store1\", 300),\n",
    "(\"2023-01-01\", \"Store2\", 50),\n",
    "(\"2023-01-02\", \"Store2\", 60),\n",
    "(\"2023-01-03\", \"Store2\", 80),\n",
    "(\"2023-01-04\", \"Store2\", 90),\n",
    "(\"2023-01-05\", \"Store2\", 120)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Date\", \"Store\", \"Sales\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(\"Date\", to_date(df.Date, 'yyyy-MM-dd'))\n",
    "\n",
    "windowSpec = Window.partitionBy(\"Store\").orderBy(\"Date\")\n",
    "\n",
    "df = df.withColumn(\"Lag_Sales\", lag(df[\"Sales\"]).over(windowSpec))\n",
    "df = df.withColumn(\"Lead_Sales\", lead(df[\"Sales\"]).over(windowSpec))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c475a676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|Column1|Column2|Column3|\n",
      "+-------+-------+-------+\n",
      "|      1|      2|      3|\n",
      "|      2|      3|      4|\n",
      "|      1|      2|      3|\n",
      "|      4|      5|      6|\n",
      "|      2|      3|      4|\n",
      "+-------+-------+-------+\n",
      "\n",
      "+-------------+-----+\n",
      "|single_column|count|\n",
      "+-------------+-----+\n",
      "|            2|    4|\n",
      "|            3|    4|\n",
      "|            4|    3|\n",
      "|            1|    2|\n",
      "|            5|    1|\n",
      "|            6|    1|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "data = [(1, 2, 3),\n",
    "(2, 3, 4),\n",
    "(1, 2, 3),\n",
    "(4, 5, 6),\n",
    "(2, 3, 4)]\n",
    "df = spark.createDataFrame(data, [\"Column1\", \"Column2\", \"Column3\"])\n",
    "\n",
    "# Print DataFrame\n",
    "df.show()\n",
    "\n",
    "columns = df.columns\n",
    "\n",
    "df_single = None\n",
    "\n",
    "for c in columns:\n",
    "    if df_single is None:\n",
    "        df_single = df.select(col(c).alias(\"single_column\"))\n",
    "    else:\n",
    "        df_single = df_single.union(df.select(col(c).alias(\"single_column\")))\n",
    "\n",
    "frequency_table = df_single.groupBy(\"single_column\").count().orderBy('count', ascending=False)\n",
    "\n",
    "frequency_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ccefb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+\n",
      "|col_1|col_2|col_3|col_4|\n",
      "+-----+-----+-----+-----+\n",
      "|    1|    2|    3|    4|\n",
      "|    2|    3|    4|    5|\n",
      "|    1|    2|    3|    4|\n",
      "|    4|    5|    6|    7|\n",
      "+-----+-----+-----+-----+\n",
      "\n",
      "+-----+-----+-----+-----+\n",
      "|col_1|col_2|col_3|col_4|\n",
      "+-----+-----+-----+-----+\n",
      "|    0|    2|    3|    0|\n",
      "|    2|    0|    0|    5|\n",
      "|    1|    0|    0|    4|\n",
      "|    0|    5|    6|    0|\n",
      "+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "data = [(1, 2, 3, 4),\n",
    "(2, 3, 4, 5),\n",
    "(1, 2, 3, 4),\n",
    "(4, 5, 6, 7)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"col_1\", \"col_2\", \"col_3\", \"col_4\"])\n",
    "\n",
    "# Print DataFrame\n",
    "df.show()\n",
    "\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "\n",
    "df = df.withColumn(\"id\", row_number().over(w) - 1)\n",
    "\n",
    "df = df.select([when(col(\"id\") == i, 0).otherwise(col(\"col_\"+str(i+1))).alias(\"col_\"+str(i+1)) for i in range(4)])\n",
    "\n",
    "df = df.withColumn(\"id\", row_number().over(w) - 1)\n",
    "df = df.withColumn(\"id_2\", df.count() - 1 - df[\"id\"])\n",
    "\n",
    "df_with_diag_zero = df.select([when(col(\"id_2\") == i, 0).otherwise(col(\"col_\"+str(i+1))).alias(\"col_\"+str(i+1)) for i in range(4)])\n",
    "\n",
    "df_with_diag_zero.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95bbb394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+\n",
      "|col_1|col_2|col_3|col_4|\n",
      "+-----+-----+-----+-----+\n",
      "|    1|    2|    3|    4|\n",
      "|    2|    3|    4|    5|\n",
      "|    3|    4|    5|    6|\n",
      "|    4|    5|    6|    7|\n",
      "+-----+-----+-----+-----+\n",
      "\n",
      "+-----+-----+-----+-----+\n",
      "|col_1|col_2|col_3|col_4|\n",
      "+-----+-----+-----+-----+\n",
      "|    4|    5|    6|    7|\n",
      "|    3|    4|    5|    6|\n",
      "|    2|    3|    4|    5|\n",
      "|    1|    2|    3|    4|\n",
      "+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "\n",
    "data = [(1, 2, 3, 4),\n",
    "(2, 3, 4, 5),\n",
    "(3, 4, 5, 6),\n",
    "(4, 5, 6, 7)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"col_1\", \"col_2\", \"col_3\", \"col_4\"])\n",
    "\n",
    "# Print DataFrame\n",
    "df.show()\n",
    "\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "df = df.withColumn(\"id\", row_number().over(w) - 1)\n",
    "df_2 = df.orderBy(\"id\", ascending=False).drop(\"id\")\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d1a0e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|Categories|Value|\n",
      "+----------+-----+\n",
      "|         A|   10|\n",
      "|         A|   20|\n",
      "|         B|   30|\n",
      "|         B|   20|\n",
      "|         B|   30|\n",
      "|         C|   40|\n",
      "|         C|   10|\n",
      "|         D|   10|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+-----------------+\n",
      "|Categories|Value|Categories_onehot|\n",
      "+----------+-----+-----------------+\n",
      "|A         |10   |(3,[1],[1.0])    |\n",
      "|A         |20   |(3,[1],[1.0])    |\n",
      "|B         |30   |(3,[0],[1.0])    |\n",
      "|B         |20   |(3,[0],[1.0])    |\n",
      "|B         |30   |(3,[0],[1.0])    |\n",
      "|C         |40   |(3,[2],[1.0])    |\n",
      "|C         |10   |(3,[2],[1.0])    |\n",
      "|D         |10   |(3,[],[])        |\n",
      "+----------+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "\n",
    "data = [(\"A\", 10),(\"A\", 20),(\"B\", 30),(\"B\", 20),(\"B\", 30),(\"C\", 40),(\"C\", 10),(\"D\", 10)]\n",
    "columns = [\"Categories\", \"Value\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Categories\", outputCol=\"Categories_Indexed\")\n",
    "indexerModel = indexer.fit(df)\n",
    "\n",
    "indexed_df = indexerModel.transform(df)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"Categories_Indexed\", outputCol=\"Categories_onehot\")\n",
    "encoded_df = encoder.fit(indexed_df).transform(indexed_df)\n",
    "encoded_df = encoded_df.drop(\"Categories_Indexed\")\n",
    "encoded_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46d4ad42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+----+\n",
      "|year|quarter|  EU|  US|\n",
      "+----+-------+----+----+\n",
      "|2021|      2|4500|5500|\n",
      "|2021|      1|4000|5000|\n",
      "|2021|      3|5000|6000|\n",
      "|2021|      4|6000|7000|\n",
      "+----+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "(2021, 1, \"US\", 5000),\n",
    "(2021, 1, \"EU\", 4000),\n",
    "(2021, 2, \"US\", 5500),\n",
    "(2021, 2, \"EU\", 4500),\n",
    "(2021, 3, \"US\", 6000),\n",
    "(2021, 3, \"EU\", 5000),\n",
    "(2021, 4, \"US\", 7000),\n",
    "(2021, 4, \"EU\", 6000),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"year\", \"quarter\", \"region\", \"revenue\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "pivot_df = df.groupBy(\"year\", \"quarter\").pivot(\"region\").sum(\"revenue\")\n",
    "\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c3dc252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+----+\n",
      "|year|quarter|  EU|  US|\n",
      "+----+-------+----+----+\n",
      "|2021|      2|4500|5500|\n",
      "|2021|      1|4000|5000|\n",
      "|2021|      3|5000|6000|\n",
      "|2021|      4|6000|7000|\n",
      "+----+-------+----+----+\n",
      "\n",
      "+----+-------+------+-------+\n",
      "|year|quarter|region|revenue|\n",
      "+----+-------+------+-------+\n",
      "|2021|      2|    EU|   4500|\n",
      "|2021|      2|    US|   5500|\n",
      "|2021|      1|    EU|   4000|\n",
      "|2021|      1|    US|   5000|\n",
      "|2021|      3|    EU|   5000|\n",
      "|2021|      3|    US|   6000|\n",
      "|2021|      4|    EU|   6000|\n",
      "|2021|      4|    US|   7000|\n",
      "+----+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "data = [(2021, 2, 4500, 5500),\n",
    "(2021, 1, 4000, 5000),\n",
    "(2021, 3, 5000, 6000),\n",
    "(2021, 4, 6000, 7000)]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"year\", \"quarter\", \"EU\", \"US\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()\n",
    "\n",
    "unpivotExpr = \"stack(2, 'EU',EU, 'US', US) as (region,revenue)\"\n",
    "\n",
    "unPivotDF = pivot_df.select(\"year\",\"quarter\", expr(unpivotExpr)).where(\"revenue is not null\")\n",
    "\n",
    "unPivotDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a346adac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "|   1|NULL|\n",
      "|NULL|   2|\n",
      "|   3|   4|\n",
      "|   5|NULL|\n",
      "+----+----+\n",
      "\n",
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  0|\n",
      "|  0|  2|\n",
      "|  3|  4|\n",
      "|  5|  0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, None), (None, 2), (3, 4), (5, None)], [\"a\", \"b\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df_imputed = df.fillna(0)\n",
    "\n",
    "df_imputed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f652edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----------+---------+------+---+------+--------+-------------+---------+--------------+---------------+------+\n",
      "|RowNumber|CustomerId|Surname |CreditScore|Geography|Gender|Age|Tenure|Balance |NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|Exited|\n",
      "+---------+----------+--------+-----------+---------+------+---+------+--------+-------------+---------+--------------+---------------+------+\n",
      "|1        |15634602  |Hargrave|619        |France   |Female|42 |2     |0.0     |1            |1        |1             |101348.88      |1     |\n",
      "|2        |15647311  |Hill    |608        |Spain    |Female|41 |1     |83807.86|1            |0        |1             |112542.58      |0     |\n",
      "+---------+----------+--------+-----------+---------+------+---+------+--------+-------------+---------+--------------+---------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Anaconda\\envs\\Python311\\Lib\\site-packages\\pyspark\\sql\\functions.py:3796: FutureWarning: Deprecated in 2.1, use approx_count_distinct instead.\n",
      "  warnings.warn(\"Deprecated in 2.1, use approx_count_distinct instead.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RowNumber', 'CustomerId', 'Surname', 'CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'EstimatedSalary']\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/selva86/datasets/master/Churn_Modelling_m.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "df = spark.read.csv(SparkFiles.get(\"Churn_Modelling_m.csv\"), header=True, inferSchema=True)\n",
    "\n",
    "df.show(2, truncate=False)\n",
    "\n",
    "from pyspark.sql.types import IntegerType, StringType, NumericType\n",
    "from pyspark.sql.functions import approxCountDistinct\n",
    "\n",
    "def detect_continuous_variables(df, distinct_threshold):\n",
    "    continuous_columns = []\n",
    "    for column in df.columns:\n",
    "        dtype = df.schema[column].dataType\n",
    "        if isinstance(dtype, (IntegerType, NumericType)):\n",
    "            distinct_count = df.select(approxCountDistinct(column)).collect()[0][0]\n",
    "        if distinct_count > distinct_threshold:\n",
    "            continuous_columns.append(column)\n",
    "    return continuous_columns\n",
    "\n",
    "continuous_variables = detect_continuous_variables(df, 10)\n",
    "print(continuous_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "204847a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   2|   2|   3|\n",
      "|   2|   2|   4|\n",
      "|   1|   2|   3|\n",
      "|   1|   1|   3|\n",
      "+----+----+----+\n",
      "\n",
      "+----+-----+\n",
      "|col2|count|\n",
      "+----+-----+\n",
      "|   2|    4|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "data = [(1, 2, 3), (2, 2, 3), (2, 2, 4), (1, 2, 3), (1, 1, 3)]\n",
    "columns = [\"col1\", \"col2\", \"col3\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()\n",
    "df_grouped = df.groupBy('col2').count()\n",
    "mode_df = df_grouped.orderBy(col('count').desc()).limit(1)\n",
    "\n",
    "mode_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1b4551e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:\\Anaconda\\envs\\Python311\\Lib\\site-packages\\pyspark\n",
      "f:\\Anaconda\\envs\\Python311\\Lib\\site-packages\\pyspark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "print(findspark.find())\n",
    "\n",
    "import os\n",
    "import pyspark\n",
    "\n",
    "print(os.path.dirname(pyspark.__file__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "35081be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|        Name|         City|\n",
      "+------------+-------------+\n",
      "|    John Doe|     NEW YORK|\n",
      "|    Jane Doe|  LOS ANGELES|\n",
      "|Mike Johnson|      CHICAGO|\n",
      "|  Sara Smith|SAN FRANCISCO|\n",
      "+------------+-------------+\n",
      "\n",
      "+------------+-------------+-------------+\n",
      "|        Name|         City|   City_lower|\n",
      "+------------+-------------+-------------+\n",
      "|    John Doe|     NEW YORK|     new york|\n",
      "|    Jane Doe|  LOS ANGELES|  los angeles|\n",
      "|Mike Johnson|      CHICAGO|      chicago|\n",
      "|  Sara Smith|SAN FRANCISCO|san francisco|\n",
      "+------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "data = [('John Doe', 'NEW YORK'),\n",
    "('Jane Doe', 'LOS ANGELES'),\n",
    "('Mike Johnson', 'CHICAGO'),\n",
    "('Sara Smith', 'SAN FRANCISCO')]\n",
    "\n",
    "df = spark.createDataFrame(data, ['Name', 'City'])\n",
    "\n",
    "df.show()\n",
    "\n",
    "def to_lower(s):\n",
    "    if s is not None:\n",
    "        return s.lower()\n",
    "\n",
    "udf_to_lower = udf(to_lower, StringType())\n",
    "\n",
    "df = df.withColumn('City_lower', udf_to_lower(df['City']))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8189878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|        Name|         City|\n",
      "+------------+-------------+\n",
      "|    John Doe|     NEW YORK|\n",
      "|    Jane Doe|  LOS ANGELES|\n",
      "|Mike Johnson|      CHICAGO|\n",
      "|  Sara Smith|SAN FRANCISCO|\n",
      "+------------+-------------+\n",
      "\n",
      "           Name           City\n",
      "0      John Doe       NEW YORK\n",
      "1      Jane Doe    LOS ANGELES\n",
      "2  Mike Johnson        CHICAGO\n",
      "3    Sara Smith  SAN FRANCISCO\n"
     ]
    }
   ],
   "source": [
    "data = [('John Doe', 'NEW YORK'),\n",
    "('Jane Doe', 'LOS ANGELES'),\n",
    "('Mike Johnson', 'CHICAGO'),\n",
    "('Sara Smith', 'SAN FRANCISCO')]\n",
    "\n",
    "pysparkDF = spark.createDataFrame(data, ['Name', 'City'])\n",
    "\n",
    "pysparkDF.show()\n",
    "\n",
    "pandasDF = pysparkDF.toPandas()\n",
    "\n",
    "print(pandasDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ecf0eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://DESKTOP-M1LGPP1:4040\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d16e62c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.driver.extraJavaOptions : -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.driver.host : DESKTOP-M1LGPP1\n",
      "spark.app.submitTime : 1764111439454\n",
      "spark.app.id : local-1764111440460\n",
      "spark.executor.id : driver\n",
      "spark.app.name : Pyspark 101 exercises\n",
      "spark.sql.warehouse.dir : file:/C:/Users/Michal/Documents/STUDIA%20INFORMATYKA/BigData/Zajcia%2025.11.2025/spark-warehouse\n",
      "spark.driver.port : 55529\n",
      "spark.rdd.compress : True\n",
      "spark.executor.extraJavaOptions : -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.serializer.objectStreamReset : 100\n",
      "spark.master : local[*]\n",
      "spark.submit.pyFiles : \n",
      "spark.submit.deployMode : client\n",
      "spark.ui.showConsoleProgress : true\n",
      "spark.app.startTime : 1764111439568\n"
     ]
    }
   ],
   "source": [
    "for k,v in spark.sparkContext.getConf().getAll():\n",
    "    print(f\"{k} : {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e1a8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "sc.stop()\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.executor.cores\", \"2\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d360d2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, City: string, City_lower: string]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d04b7a96",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'randomSplit'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_data, test_data = \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandomSplit\u001b[49m([\u001b[32m0.8\u001b[39m, \u001b[32m0.2\u001b[39m], seed=\u001b[32m42\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'randomSplit'"
     ]
    }
   ],
   "source": [
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d54ce93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.020277740475786732,-1.6129609400223655]\n",
      "Intercept: -0.22092927518295352\n"
     ]
    }
   ],
   "source": [
    "data = spark.createDataFrame([\n",
    "(0, 1.0, -1.0),\n",
    "(1, 2.0, 1.0),\n",
    "(1, 3.0, -2.0),\n",
    "(0, 4.0, 1.0),\n",
    "(1, 5.0, -3.0),\n",
    "(0, 6.0, 2.0),\n",
    "(1, 7.0, -1.0),\n",
    "(0, 8.0, 3.0),\n",
    "(1, 9.0, -2.0),\n",
    "(0, 10.0, 2.0),\n",
    "(1, 11.0, -3.0),\n",
    "(0, 12.0, 1.0),\n",
    "(1, 13.0, -1.0),\n",
    "(0, 14.0, 2.0),\n",
    "(1, 15.0, -2.0),\n",
    "(0, 16.0, 3.0),\n",
    "(1, 17.0, -3.0),\n",
    "(0, 18.0, 1.0),\n",
    "(1, 19.0, -1.0),\n",
    "(0, 20.0, 2.0)\n",
    "], [\"label\", \"feat1\", \"feat2\"])\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=['feat1', 'feat2'], outputCol=\"features\")\n",
    "data = vecAssembler.transform(data)\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "lr_model = lr.fit(data)\n",
    "\n",
    "print(f\"Coefficients: {str(lr_model.coefficients)}\")\n",
    "print(f\"Intercept: {str(lr_model.intercept)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8d64ddd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|animal|\n",
      "+------+\n",
      "|   cat|\n",
      "|   dog|\n",
      "| mouse|\n",
      "|  fish|\n",
      "|   dog|\n",
      "|   cat|\n",
      "| mouse|\n",
      "+------+\n",
      "\n",
      "+------+-----------+\n",
      "|animal|animalIndex|\n",
      "+------+-----------+\n",
      "|   cat|        0.0|\n",
      "|   dog|        1.0|\n",
      "| mouse|        2.0|\n",
      "|  fish|        3.0|\n",
      "|   dog|        1.0|\n",
      "|   cat|        0.0|\n",
      "| mouse|        2.0|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('cat',), ('dog',), ('mouse',), ('fish',), ('dog',), ('cat',), ('mouse',)]\n",
    "df = spark.createDataFrame(data, [\"animal\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol='animal', outputCol='animalIndex')\n",
    "\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85a076e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+\n",
      "|feature1|feature2|feature3|\n",
      "+--------+--------+--------+\n",
      "|       5|      10|      25|\n",
      "|       6|      15|      35|\n",
      "|       7|      25|      30|\n",
      "|       8|      20|      60|\n",
      "|       9|      30|      70|\n",
      "+--------+--------+--------+\n",
      "\n",
      "Correlation between feature1 and feature2 : 0.9\n"
     ]
    }
   ],
   "source": [
    "data = [Row(feature1=5, feature2=10, feature3=25),\n",
    "Row(feature1=6, feature2=15, feature3=35),\n",
    "Row(feature1=7, feature2=25, feature3=30),\n",
    "Row(feature1=8, feature2=20, feature3=60),\n",
    "Row(feature1=9, feature2=30, feature3=70)]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.show()\n",
    "\n",
    "correlation = df.corr(\"feature1\", \"feature2\")\n",
    "\n",
    "print(\"Correlation between feature1 and feature2 :\", correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1f021845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+\n",
      "|feature1|feature2|feature3|\n",
      "+--------+--------+--------+\n",
      "|       5|      10|      25|\n",
      "|       6|      15|      35|\n",
      "|       7|      25|      30|\n",
      "|       8|      20|      60|\n",
      "|       9|      30|      70|\n",
      "+--------+--------+--------+\n",
      "\n",
      "DenseMatrix([[1.        , 0.9       , 0.91779992],\n",
      "             [0.9       , 1.        , 0.67837385],\n",
      "             [0.91779992, 0.67837385, 1.        ]])\n"
     ]
    }
   ],
   "source": [
    "data = [Row(feature1=5, feature2=10, feature3=25),\n",
    "Row(feature1=6, feature2=15, feature3=35),\n",
    "Row(feature1=7, feature2=25, feature3=30),\n",
    "Row(feature1=8, feature2=20, feature3=60),\n",
    "Row(feature1=9, feature2=30, feature3=70)]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.show()\n",
    "\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "vector_assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\", \"feature3\"], outputCol=\"features\")\n",
    "data_vector = vector_assembler.transform(df).select(\"features\")\n",
    "\n",
    "correlation_matrix = Correlation.corr(data_vector, \"features\").head()[0]\n",
    "\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e359cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+\n",
      "|feature1|feature2|feature3|\n",
      "+--------+--------+--------+\n",
      "|       5|      10|      25|\n",
      "|       6|      15|      35|\n",
      "|       7|      25|      30|\n",
      "|       8|      20|      60|\n",
      "|       9|      30|      70|\n",
      "+--------+--------+--------+\n",
      "\n",
      "VIF for feature3: 23.30468749999992\n"
     ]
    }
   ],
   "source": [
    "data = [Row(feature1=5, feature2=10, feature3=25),\n",
    "Row(feature1=6, feature2=15, feature3=35),\n",
    "Row(feature1=7, feature2=25, feature3=30),\n",
    "Row(feature1=8, feature2=20, feature3=60),\n",
    "Row(feature1=9, feature2=30, feature3=70)]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.show()\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def calculate_vif(data, features):\n",
    "    vif_dict = {}\n",
    "\n",
    "    for feature in features:\n",
    "        non_feature_cols = [col for col in features if col != feature]\n",
    "        assembler = VectorAssembler(inputCols=non_feature_cols, outputCol=\"features\")\n",
    "        lr = LinearRegression(featuresCol='features', labelCol=feature)\n",
    "\n",
    "    model = lr.fit(assembler.transform(data))\n",
    "    vif = 1 / (1 - model.summary.r2)\n",
    "\n",
    "    vif_dict[feature] = vif\n",
    "\n",
    "    return vif_dict\n",
    "\n",
    "features = ['feature1', 'feature2', 'feature3']\n",
    "vif_values = calculate_vif(df, features)\n",
    "\n",
    "for feature, vif in vif_values.items():\n",
    "    print(f'VIF for {feature}: {vif}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4549b556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+--------+-----+\n",
      "| id|feature1|feature2|feature3|label|\n",
      "+---+--------+--------+--------+-----+\n",
      "|  1|       0|       0|       1|    1|\n",
      "|  2|       0|       1|       0|    0|\n",
      "|  3|       1|       0|       0|    0|\n",
      "|  4|       0|       0|       1|    1|\n",
      "|  5|       0|       1|       1|    0|\n",
      "+---+--------+--------+--------+-----+\n",
      "\n",
      "pValues: [0.36131042852617856,0.13603712811414348,0.1360371281141436]\n",
      "degreesOfFreedom: [1, 1, 1]\n",
      "statistics: [0.8333333333333335,2.2222222222222228,2.2222222222222223]\n"
     ]
    }
   ],
   "source": [
    "# Create a sample dataframe\n",
    "data = [(1, 0, 0, 1, 1),\n",
    "(2, 0, 1, 0, 0),\n",
    "(3, 1, 0, 0, 0),\n",
    "(4, 0, 0, 1, 1),\n",
    "(5, 0, 1, 1, 0)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"feature1\", \"feature2\", \"feature3\", \"label\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\", \"feature3\"], outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0db47b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n",
      "|Employee|Department|Salary|\n",
      "+--------+----------+------+\n",
      "|   James|     Sales|  3000|\n",
      "| Michael|     Sales|  4600|\n",
      "|  Robert|     Sales|  4100|\n",
      "|   Maria|   Finance|  3000|\n",
      "|   James|     Sales|  3000|\n",
      "|   Scott|   Finance|  3300|\n",
      "|     Jen|   Finance|  3900|\n",
      "|    Jeff| Marketing|  3000|\n",
      "|   Kumar| Marketing|  2000|\n",
      "|    Saif|     Sales|  4100|\n",
      "+--------+----------+------+\n",
      "\n",
      "+-----------------+\n",
      "|           stddev|\n",
      "+-----------------+\n",
      "|765.9416862050705|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [(\"James\", \"Sales\", 3000),\n",
    "(\"Michael\", \"Sales\", 4600),\n",
    "(\"Robert\", \"Sales\", 4100),\n",
    "(\"Maria\", \"Finance\", 3000),\n",
    "(\"James\", \"Sales\", 3000),\n",
    "(\"Scott\", \"Finance\", 3300),\n",
    "(\"Jen\", \"Finance\", 3900),\n",
    "(\"Jeff\", \"Marketing\", 3000),\n",
    "(\"Kumar\", \"Marketing\", 2000),\n",
    "(\"Saif\", \"Sales\", 4100)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"Employee\", \"Department\", \"Salary\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "from pyspark.sql.functions import stddev\n",
    "\n",
    "salary_stddev = df.select(stddev(\"Salary\").alias(\"stddev\"))\n",
    "\n",
    "salary_stddev.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1556807f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------+\n",
      "|FirstName|LastName|    City|\n",
      "+---------+--------+--------+\n",
      "|     John|     Doe|    NULL|\n",
      "|     NULL|   Smith|New York|\n",
      "|     Mike|   Smith|    NULL|\n",
      "|     Anna|   Smith|  Boston|\n",
      "|     NULL|    NULL|    NULL|\n",
      "+---------+--------+--------+\n",
      "\n",
      "Missing values in City: 60.0%\n"
     ]
    }
   ],
   "source": [
    "# Create a sample dataframe\n",
    "data = [(\"John\", \"Doe\", None),\n",
    "(None, \"Smith\", \"New York\"),\n",
    "(\"Mike\", \"Smith\", None),\n",
    "(\"Anna\", \"Smith\", \"Boston\"),\n",
    "(None, None, None)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"FirstName\", \"LastName\", \"City\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "total_rows = df.count()\n",
    "\n",
    "for column in df.columns:\n",
    "    null_values = df.filter(df[column].isNull()).count()\n",
    "    missing_percentage = (null_values / total_rows) * 100\n",
    "print(f\"Missing values in {column}: {missing_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e8648a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "df\n",
      "missing\n",
      "imputed_df\n",
      "df_last_2\n",
      "df_assembled\n",
      "df_normalized\n",
      "df_single\n",
      "frequency_table\n",
      "df_with_diag_zero\n",
      "df_2\n",
      "indexed_df\n",
      "encoded_df\n",
      "pivot_df\n",
      "unPivotDF\n",
      "df_imputed\n",
      "df_grouped\n",
      "mode_df\n",
      "pysparkDF\n",
      "_50\n",
      "indexed\n",
      "data_vector\n",
      "salary_stddev\n"
     ]
    }
   ],
   "source": [
    "dataframe_names = [name for name, obj in globals().items() if isinstance(obj, pyspark.sql.DataFrame)]\n",
    "\n",
    "for name in dataframe_names:\n",
    "    print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
